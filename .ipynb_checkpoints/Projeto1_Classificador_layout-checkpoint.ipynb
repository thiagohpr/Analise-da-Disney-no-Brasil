{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Pedro de Souza Zequi\n",
    "\n",
    "Nome: Thiago Hampl de Pierri Rocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atenção:** Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Em `filename`, coloque o nome do seu arquivo de dados!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrei o arquivo tweetsbrasil.xlsx, tudo certo para prosseguir com a prova!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filename = 'tweetsbrasil.xlsx'\n",
    "if filename in os.listdir():\n",
    "    print(f'Encontrei o arquivo {filename}, tudo certo para prosseguir com a prova!')\n",
    "else:\n",
    "    print(f'Não encontrei o arquivo {filename} aqui no diretório {os.getcwd()}, será que você não baixou o arquivo?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@aassemblebr apaga aabr, se não o crivella der...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>rt @lap_canalnerd: e está confirmado, finalmen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>exclusivo: sony homologa controle de playstati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>rt @egirao: @hscpedro\\nviu isso?\\nhttps://t.co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>q saco a disney plus vai demorar ainda pra che...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relevancia\n",
       "0  @aassemblebr apaga aabr, se não o crivella der...           1\n",
       "1  rt @lap_canalnerd: e está confirmado, finalmen...           1\n",
       "2  exclusivo: sony homologa controle de playstati...           0\n",
       "3  rt @egirao: @hscpedro\\nviu isso?\\nhttps://t.co...           0\n",
       "4  q saco a disney plus vai demorar ainda pra che...           1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel(filename)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@almanaquedisney pena que ainda não tenho disn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@harleybong @ghabbema @hugoverdose @azaghal @b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>queria acordar com a notícia que a disney ante...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>rt @leofrancisco: #disneyplus: os 66 episódios...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@lord_cwestfall @cygnetwaves @juliella_fer nos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  Relevancia\n",
       "0  @almanaquedisney pena que ainda não tenho disn...           1\n",
       "1  @harleybong @ghabbema @hugoverdose @azaghal @b...           1\n",
       "2  queria acordar com a notícia que a disney ante...           1\n",
       "3  rt @leofrancisco: #disneyplus: os 66 episódios...           1\n",
       "4  @lord_cwestfall @cygnetwaves @juliella_fer nos...           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça aqui uma descrição do seu produto e o que considerou como relevante ou não relevante na classificação dos tweets.\n",
    "\n",
    "O projeto visa estudar comentários de internautas no Twitter sobre o lançamento do serviço de streaming Disney+ no Brasil. Dessa forma, trata-se de um produto que ainda não foi lançado e, portanto, os critérios de relevância devem considerar tal fato. Em uma primeira iteração, por exemplo, houve comentários relacionados ao funcionamento e/ou perfomance do serviço. Tais comentários foram descartados pois se tratava da Disney+ em Portugal, país em que o streaming já está disponível. Comentários que citavam unicamente a produtora Disney foram igualmente irrelevantes. Dessa forma, o principal critério de relevância adotado por nós foi qualquer um que mostrasse certa expectativa (tanto positiva como negativa) em relação ao lançamento do produto no Brasil. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391\n",
      "            Palavras  Prob. Relevante  Prob. Irrelevante  Relevância\n",
      "8             disney         0.027227           0.016432           1\n",
      "10                no         0.026524           0.016589           1\n",
      "19            brasil         0.023011           0.016119           1\n",
      "9               plus         0.019147           0.004538           1\n",
      "5                  o         0.016336           0.005164           1\n",
      "..               ...              ...                ...         ...\n",
      "963        1ujscnfrt         0.000000           0.000156           0\n",
      "962      httpstcopwt         0.000000           0.000156           0\n",
      "960  starwars_direct         0.000000           0.000156           0\n",
      "959              via         0.000000           0.000156           0\n",
      "623         aventura         0.000000           0.000156           0\n",
      "\n",
      "[1314 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    #import string\n",
    "    punctuation = '[!-./\\|@:?;]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    return text_subbed\n",
    "\n",
    "\n",
    "def lista (serie):\n",
    "    #Recebe uma série ou uma frase e retorna uma lista de palavras sem símbolos,\n",
    "    #padronizadas em minúsculas e com emojis separados.\n",
    "    \n",
    "    train=\"\"\n",
    "    for frase in serie:\n",
    "        train+=frase\n",
    "    train=cleanup(train.lower())\n",
    "    palavras=tweet_tokenizer.tokenize(train)\n",
    "    return palavras\n",
    " \n",
    "def retira_repetidas (lista):\n",
    "    #função utilizada para que o modelo não calcule a probabilidade relativa da mesma palavra várias vezes.\n",
    "    nova=[]\n",
    "    for p in lista:\n",
    "        if p not in nova:\n",
    "            nova.append(p)\n",
    "    return nova\n",
    "        \n",
    "#cálculo da probabilidade da mensagem ser relevante ou irrelevante   \n",
    "tudo=train.Relevancia.value_counts(normalize=True)\n",
    "P_R=tudo[1]\n",
    "P_I=tudo[0]\n",
    "\n",
    "\n",
    "#listas com todas as palavras com todas as suas ocorrências e lista com as palavras aplicando a função que retiras as repetidas\n",
    "todaspalavrasrepetidas=lista(train.Treinamento)\n",
    "todaspalavras=retira_repetidas(todaspalavrasrepetidas)\n",
    "\n",
    "\n",
    "#series com as frases relevantes e com as frases irrelevantes\n",
    "relevante=train.loc[(train.Relevancia==1),:]\n",
    "irrelevante=train.loc[(train.Relevancia==0),:]\n",
    "frasesr=relevante.Treinamento\n",
    "frasesi=irrelevante.Treinamento\n",
    "\n",
    "\n",
    "#lista de palavras relevantes e lista de palavras irrelevantes\n",
    "pr=lista(frasesr)\n",
    "pi=lista(frasesi)\n",
    "\n",
    "\n",
    "#mesma lista porem em forma de serie para atribuir funções do pandas\n",
    "palavrasr=pd.Series(pr)\n",
    "palavrasi=pd.Series(pi)\n",
    "\n",
    "\n",
    "#frequências absolutas\n",
    "absolutr=palavrasr.value_counts()\n",
    "absoluti=palavrasi.value_counts()\n",
    "\n",
    "\n",
    "#frequências relativas\n",
    "relativar=palavrasr.value_counts(True)\n",
    "relativai=palavrasi.value_counts(True)\n",
    "\n",
    "\n",
    "data=[]\n",
    "#início do looping com as palavras sem repetição\n",
    "for p in todaspalavras:\n",
    "    #Dois ifs que verificam se a palavra está na lista das relevantes e/ou irrelevantes.\n",
    "    \n",
    "    if p in pr:\n",
    "        P_palavra_dado_R=relativar[p]\n",
    "        P_R_dado_palavra=P_palavra_dado_R * P_R\n",
    "    else:\n",
    "        #Se não estiver nas relevantes, a probabilidade da palavra nessa condição é zero.\n",
    "        P_R_dado_palavra=0\n",
    "    \n",
    "    \n",
    "    if p in pi:\n",
    "        P_palavra_dado_I=relativai[p]\n",
    "        P_I_dado_palavra=P_palavra_dado_I * P_I\n",
    "    else:\n",
    "        #Se não estiver nas irrelevantes, a probabilidade da palavra nessa condição é zero.\n",
    "        P_I_dado_palavra=0\n",
    "        \n",
    "        \n",
    "    #Verificação de qual é maior e cria um novo dataframe que estuda todas as probabilidades de cada palavra.\n",
    "    if P_R_dado_palavra>P_I_dado_palavra:\n",
    "        data.append([p,P_R_dado_palavra,P_I_dado_palavra,1])\n",
    "    elif P_I_dado_palavra>P_R_dado_palavra:\n",
    "        data.append([p,P_R_dado_palavra,P_I_dado_palavra,0])\n",
    "\n",
    "treinamento=pd.DataFrame(data, columns = ['Palavras','Prob. Relevante','Prob. Irrelevante', 'Relevância'])\n",
    "\n",
    "\n",
    "\n",
    "print (treinamento.sort_values('Prob. Relevante',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Frase  Relevância  \\\n",
      "0    @almanaquedisney pena que ainda não tenho disn...           1   \n",
      "1    @harleybong @ghabbema @hugoverdose @azaghal @b...           1   \n",
      "2    queria acordar com a notícia que a disney ante...           1   \n",
      "3    rt @leofrancisco: #disneyplus: os 66 episódios...           1   \n",
      "4    @lord_cwestfall @cygnetwaves @juliella_fer nos...           0   \n",
      "..                                                 ...         ...   \n",
      "120  a disney não vai mais produzir mídia física no...           0   \n",
      "121  rt @ladynoirdestiny: o especial de ny já tá na...           0   \n",
      "122  @bouyssounade mano no brasil as plataformas de...           0   \n",
      "123  rt @essediegoluiz: @disneyplusbr olha aqui don...           1   \n",
      "124  rt @ramsowmv: @uolesporte falta de interpretaç...           0   \n",
      "\n",
      "     Classificador  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n",
      "..             ...  \n",
      "120              1  \n",
      "121              0  \n",
      "122              1  \n",
      "123              1  \n",
      "124              0  \n",
      "\n",
      "[125 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "relevancia=test.Relevancia\n",
    "\n",
    "serie=test.Teste\n",
    "\n",
    "#Cálculo da probabilidade de ser relevante ou irrelevante no banco de dados de teste.\n",
    "tudo=relevancia.value_counts(normalize=True)\n",
    "P_Rt=tudo[1]\n",
    "P_It=tudo[0]\n",
    "\n",
    "#Criação de uma lista com cada frase na serie de teste.\n",
    "frases=[]\n",
    "for frase in serie:\n",
    "    frases.append(frase)\n",
    "\n",
    "\n",
    "#Declaração de variáveis de contagem\n",
    "vp=0\n",
    "fp=0\n",
    "vn=0\n",
    "fn=0\n",
    "\n",
    "i=0\n",
    "\n",
    "data2=[]\n",
    "#Início de um looping por frase.\n",
    "for frase in frases:\n",
    "    Pfrase_dado_R=1\n",
    "    Pfrase_dado_I=1\n",
    "    \n",
    "    #Filtro da frase com a mesma função utilizada no treinamento, para que não ocorram palavras diferentes.\n",
    "    frasefiltrada=lista(frase)\n",
    "    \n",
    "    #Antes de calcular as probabilidades, é necessário checar se existem palavras novas a serem classificadas.\n",
    "    novasr=0\n",
    "    novasi=0\n",
    "    \n",
    "    #Primeiro looping de palavras, que conta as palavras novas em relevantes e irrelevantes.\n",
    "    for p in frasefiltrada:\n",
    "        if p not in pr:\n",
    "            novasr+=1\n",
    "        if p not in pi:\n",
    "            novasi+=1\n",
    "    \n",
    "    #Se existem, aplica um booleano True para o cada caso.\n",
    "    if novasr>0:\n",
    "        smoothingr=True\n",
    "    else:\n",
    "        smoothingr=False\n",
    "        \n",
    "    if novasi>0:\n",
    "        smoothingi=True\n",
    "    else:\n",
    "        smoothingi=False\n",
    "            \n",
    "    \n",
    "    #Segundo looping de palavras da frase, agora com a informação de se o Laplace smoothing deverá ser usado ou não.\n",
    "    for p in frasefiltrada:\n",
    "    \n",
    "        if smoothingr==True:\n",
    "            #Verifica primeiro se a palavra existe na lista de palavras relevantes para evitar erros.\n",
    "            if p not in pr:\n",
    "                #Se não existir, a frequência absoluta é 0.\n",
    "                absolutor=0\n",
    "            else:\n",
    "                absolutor=absolutr[p]\n",
    "                \n",
    "            #Cálculo da probabilidade condicional da palavra usando o Laplace smoothing em relevantes.    \n",
    "            P_p_R=(absolutor+1)/(len(pr)+len(todaspalavrasrepetidas))\n",
    "            Pfrase_dado_R=Pfrase_dado_R*P_p_R\n",
    "        else:\n",
    "            #Cálculo da probabilidade condicional da palavra usando a frequência relativa da palavra em relevantes.\n",
    "            P_p_R=relativar[p]\n",
    "            Pfrase_dado_R=Pfrase_dado_R*P_p_R\n",
    "            \n",
    "            \n",
    "        if smoothingi==True:\n",
    "            #Verifica primeiro se a palavra existe na lista de palavras irrelevantes para evitar erros.\n",
    "            if p not in pi:\n",
    "                #Se não existir, a frequência absoluta é 0.\n",
    "                absolutoi=0\n",
    "            else:\n",
    "                absolutoi=absoluti[p]\n",
    "                \n",
    "            #Cálculo da probabilidade condicional da palavra usando o Laplace smoothing em irrelevantes.    \n",
    "            P_p_I=(absolutoi+1)/(len(pi)+len(todaspalavrasrepetidas))\n",
    "            Pfrase_dado_I=Pfrase_dado_I*P_p_I\n",
    "            \n",
    "        else:\n",
    "            #Cálculo da probabilidade condicional da palavra usando a frequência relativa da palavra em irrelevantes.\n",
    "            P_p_I=relativai[p]\n",
    "            Pfrase_dado_I=Pfrase_dado_I*P_p_I\n",
    "            \n",
    "        \n",
    "        \n",
    "    #Agora fora do looping das palavras, calcula a probabilidade final a partir das condicionais das palavras.    \n",
    "    P_R_dado_frase=Pfrase_dado_R*P_Rt\n",
    "    P_I_dado_frase=Pfrase_dado_I*P_It\n",
    "    \n",
    "    #Verifica qual probabilidade é maior e atribui um valor de 1 ou 0.\n",
    "    if P_R_dado_frase>P_I_dado_frase:\n",
    "        data2.append([frase,relevancia[i],1])\n",
    "    elif P_I_dado_frase>P_R_dado_frase:\n",
    "        data2.append([frase,relevancia[i],0])\n",
    "\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "#Cria um novo dataframe com a frase, relevância e relevância classificada pelo modelo.\n",
    "teste=pd.DataFrame(data2, columns = ['Frase','Relevância','Classificador']) \n",
    "\n",
    "modelo=teste.Classificador\n",
    "\n",
    "#Contagem da classificação de cada mensagem.\n",
    "for i in range (len(relevancia)):\n",
    "    if relevancia[i]==1:\n",
    "        if modelo[i]==1:\n",
    "            vp+=1\n",
    "        elif modelo[i]==0:\n",
    "            fn+=1\n",
    "    elif relevancia[i]==0:\n",
    "        if modelo[i]==1:\n",
    "            fp+=1\n",
    "        elif modelo[i]==0:\n",
    "            vn+=1\n",
    "\n",
    "#Cálculo da probabilidade de cada classificação.\n",
    "probvp=vp/len(serie)   \n",
    "probfp=fp/len(serie)\n",
    "probvn=vn/len(serie)\n",
    "probfn=fn/len(serie)\n",
    "\n",
    "\n",
    "print (teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificador   0   1\n",
      "Relevancia           \n",
      "0              26  47\n",
      "1               4  48\n",
      "\n",
      "\n",
      "A probabilidade da mensagem ser relevante e ser classificada como relevante é 38.4%\n",
      "A probabilidade da mensagem ser irrelevante e ser classificada como relevante é 37.6%\n",
      "A probabilidade da mensagem ser irrelevante e ser classificada como irrelevante é 20.8%\n",
      "A probabilidade da mensagem ser relevante e ser classificada como irrelevante é 3.2%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab (relevancia,modelo))\n",
    "print ('\\n')\n",
    "print (\"A probabilidade da mensagem ser relevante e ser classificada como relevante é {}%\".format(probvp*100))\n",
    "print (\"A probabilidade da mensagem ser irrelevante e ser classificada como relevante é {}%\".format(probfp*100))\n",
    "print (\"A probabilidade da mensagem ser irrelevante e ser classificada como irrelevante é {}%\".format(probvn*100))\n",
    "print (\"A probabilidade da mensagem ser relevante e ser classificada como irrelevante é {}%\".format(probfn*100))\n",
    "print ('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   A probabilidade de acerto do modelo (verdadeiros positivos e verdadeiros negativos) é 59.2%. À primeira vista, entende-se que o classificador possui uma boa previsão da satisfação dos tweets. No entanto, os 40.8% classificados erroneamente (falsos positivos e falsos negativos) estão centralizados nos falsos positivos. Dado que a mensagem é irrelevante, a probabilidade de acerto do modelo é de apenas 35.6%. Isso não é um valor adequado para um classificador satisfatório, pior ainda visto que a maior ocorrência de mensagens é justamente as irrelevantes no banco de dados de teste (58.4% de todos os tweets). A razão pela qual há tanta ocorrência de falsos positivos pode ser explicada pela tendência do classificador em atribuir o valor de relevância aos tweets de teste. Das 125 mensagens analisadas pelo modelo, houve um total de 95 relevantes (76% da amostra de teste). \n",
    "   Já que o grupo considerou como relevantes apenas mensagens que tratavam sobre o DisneyPlus no Brasil, em muitos tweets irrelevantes a palavra \"disney\" aparece, no entanto tratando da empresa de forma geral e não do serviço de streaming (produto escolhido pelo grupo). Na tabela relativa dos tweets relevantes, a palavra com mais ocorrência era justamente \"disney\" e, desde que esta aparece em ambos os tweets relevantes e irrelevantes, o classificador atribuiu muitos tweets irrelevantes como relevantes, sendo assim essa foi uma teoria descrita pelo grupo que poderia possivelmente explicar do alto número de falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
